main_task: "reading_comprehension"
auxiliary_task: "mask_language_model answer_matching"

do_train: True
do_predict: True

use_cuda: False

checkpoint_path: "output_model/firstrun"

backbone_model: "bert_model"
pretrain_model_path: "pretrain_model/bert"
pretrain_config_path: "pretrain_model/bert/bert_config.json"
vocab_path: "pretrain_model/bert/vocab.txt"

# backbone_model: "ernie_model"
# pretrain_model_path: "pretrain_model/ernie/params"
# pretrain_config_path: "pretrain_model/ernie/ernie_config.json"
# vocab_path: "pretrain_model/ernie/vocab.txt"

optimizer: "bert_optimizer"
learning_rate: 3e-5
lr_scheduler: "linear_warmup_decay"
skip_steps: 10
save_steps: 10000
epoch: 2
warmup_proportion: 0.1
weight_decay: 0.1
do_lower_case: True
max_seq_len: 512
use_ema: True
ema_decay: 0.9999
random_seed: 0
loss_scaling: 1.0

